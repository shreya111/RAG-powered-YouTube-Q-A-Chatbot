# -*- coding: utf-8 -*-
"""chat bot gen ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15KlH4e4PQdWjvduzEZduKk562TN82wwl
"""

# RAG-based Gen-AI Chatbot: YouTube Q&A

# Features:
#  - Topic selection
#  - Transcript fetching via YouTube Transcript API or captions
#  - Whisper fallback transcription
#  - Text splitting
#  - HF embeddings + Chroma vector DB
#  - Conversational RAG with memory
#  - Gradio UI with a simple chat loop (type 'exit' to end)
# ---------------------------------------------------------------

# ==============================
# Step 0: Install Dependencies
# ==============================
# (Uncomment these when running in Colab/Notebook)
!pip install -q yt-dlp gradio langchain langchain-community langchain-huggingface chromadb
!pip install -q transformers sentence-transformers huggingface_hub youtube-transcript-api
!pip install -q git+https://github.com/openai/whisper.git
!apt-get -y install -qq ffmpeg

# ==================
# Step 1: Imports
# ==================
import os
import tempfile
import yt_dlp
import shutil
import requests
import whisper
import gradio as gr

from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

# LLM backends (Hugging Face)
from langchain_community.llms import HuggingFaceEndpoint, HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# ==============================
# Step 2: Set HF API Token (optional for Inference API)

from google.colab import userdata
# userdata = {"OPEN_API_KEY": "hf_..."}
os.environ["HUGGINGFACEHUB_API_TOKEN"] = userdata.get("OPEN_API_KEY")

HF_TOKEN = os.environ.get("HUGGINGFACEHUB_API_TOKEN", "OPEN_API_KEY")

# ======================================================
# Step 3: YouTube Transcript API (transcript fetching)
# ======================================================
from youtube_transcript_api import YouTubeTranscriptApi

def get_youtube_transcript_via_api(video_id: str) -> str:
    try:
        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)
        # Try to get the English transcript first
        transcript = transcript_list.find_transcript(['en'])
        return " ".join([item["text"] for item in transcript.fetch()])
    except (TranscriptsDisabled, NoTranscriptFound):
        return ""
    except Exception as e:
        print(f"Error fetching transcript via API for {video_id}: {e}")
        return ""

# ======================================================
# Step 4: yt-dlp captions fallback (captions fetching)
# ======================================================
COOKIE_PATH = "" # Optional: Set path to a cookies file if needed for private videos

def get_youtube_captions_via_ytdlp(video_id: str) -> str:
    try:
        # Use yt-dlp to extract info, including captions
        ydl_opts = {
            'writesubtitles': True,
            'writeautomaticsub': True,
            'skip_download': True,
            'format': 'best',
            'quiet': True,
        }
        if COOKIE_PATH and os.path.exists(COOKIE_PATH):
            ydl_opts["cookiefile"] = COOKIE_PATH

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info_dict = ydl.extract_info(f"https://www.youtube.com/watch?v={video_id}", download=False)
            # Find an English subtitle track (manual or automatic)
            subtitles = info_dict.get('subtitles') or info_dict.get('automatic_captions')
            if not subtitles:
                return ""

            en_subs = None
            # Prioritize manual English ('en') over automatic ('en-AU', 'en-GB', etc.)
            if 'en' in subtitles:
                en_subs = subtitles['en']
            else:
                 # Look for any English variant
                 for lang_code, sub_list in subtitles.items():
                     if lang_code.startswith('en'):
                         en_subs = sub_list
                         break

            if not en_subs:
                return ""

            # Get the URL of the first subtitle format (often vtt or srv1)
            sub_url = en_subs[0]['url'] # srv1 format is usually better structured

            # Download the subtitle content
            response = requests.get(sub_url)
            response.raise_for_status() # Raise an exception for bad status codes

            # Basic parsing (can be improved for better accuracy)
            # This is a simple approach for srv1 format
            if '.srv1' in sub_url:
                 # Simplified parsing for srv1 structure
                 # Assuming lines are like: <text start="TIME">TEXT</text>
                 import re
                 text_lines = re.findall(r'<text start="\d+\.\d+">\s*(.*?)\s*</text>', response.text)
                 return " ".join(text_lines)
            else:
                 # For VTT or other formats, might need a proper parser
                 # Simple fallback: remove timing info lines
                 lines = response.text.splitlines()
                 transcript_lines = [line for line in lines if "-->" not in line and not line.strip().isdigit() and line.strip() != ""]
                 return " ".join(transcript_lines)


    except Exception as e:
        print(f"Error fetching captions via yt-dlp for {video_id}: {e}")
        return ""

# ======================================================
# Step 5: Whisper fallback transcription (audio -> text)
# ======================================================
whisper_model = whisper.load_model("base")


def transcribe_youtube_audio(video_url: str) -> str:
    with tempfile.TemporaryDirectory() as tmpdir:
        ydl_opts = {
            "format": "bestaudio/best",
            "outtmpl": f"{tmpdir}/audio.%(ext)s",
            "quiet": True,
            "postprocessors": [
                {
                    "key": "FFmpegExtractAudio",
                    "preferredcodec": "mp3",
                    "preferredquality": "192",
                }
            ],
        }
        if COOKIE_PATH and os.path.exists(COOKIE_PATH):
            ydl_opts["cookiefile"] = COOKIE_PATH

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([video_url])
            audio_path = os.path.join(tmpdir, "audio.mp3")
            result = whisper_model.transcribe(audio_path)
            return result.get("text", "")

# ======================================================
# Step 6: Unified transcript fetcher (API -> captions -> Whisper)
# ======================================================

def extract_video_id(url_or_id: str) -> str:
    if "youtube.com" in url_or_id or "youtu.be" in url_or_id:
        # Basic parsing of v= or short URL
        if "v=" in url_or_id:
            return url_or_id.split("v=")[-1].split("&")[0]
        if "youtu.be/" in url_or_id:
            return url_or_id.split("youtu.be/")[-1].split("?")[0]
    return url_or_id


def fetch_full_transcript(url_or_id: str) -> str:
    vid = extract_video_id(url_or_id)

    # 1) YouTube Transcript API
    t = get_youtube_transcript_via_api(vid)
    if t and len(t.strip()) > 100:
        return t

    # 2) yt-dlp captions
    t = get_youtube_captions_via_ytdlp(vid)
    if t and len(t.strip()) > 100:
        return t

    # 3) Whisper fallback
    return transcribe_youtube_audio(f"https://www.youtube.com/watch?v={vid}")

# =====================================
# Step 7: Memory and globals
# =====================================
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
qa_chain = None
conversation_ended = False
current_topic = "General"

# ======================================================
# Step 8: Build Retrieval-Augmented Generation Chain
# ======================================================
#  - Split text
#  - HF embeddings -> Chroma
#  - LLM via HuggingFaceEndpoint (Inference API) OR local pipeline fallback

def build_llm():
    """Return a LangChain-compatible LLM using Hugging Face.
    Tries Inference API first (requires HF_TOKEN), then local Transformers pipeline.
    """
    # Preferred: Use Inference API hosted model (fast, simple). Requires an access token with rights.
    if HF_TOKEN:
        try:
            # Good instruct models on HF Inference API (adjust as you like):
            # - meta-llama/Meta-Llama-3.1-8B-Instruct
            # - mistralai/Mixtral-8x7B-Instruct-v0.1
            # - google/gemma-2-9b-it
            llm = HuggingFaceEndpoint(
                repo_id="meta-llama/Meta-Llama-3.1-8B-Instruct",
                temperature=0.2,
                max_new_tokens=512,
                huggingfacehub_api_token=HF_TOKEN,
            )
            _ = llm("Ping")  # quick sanity check
            print("Using Hugging Face Inference API.")
            return llm
        except Exception as e:
            print("HF Endpoint init failed, falling back to local pipeline:", e)

    # Fallback: local/Colab pipeline (CPU/GPU) â€” choose a small instruct model
    # Note: Larger models will be slow and memory-heavy on CPU-only sessions.
    local_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    try:
        tok = AutoTokenizer.from_pretrained(local_model_id)
        mdl = AutoModelForCausalLM.from_pretrained(local_model_id)
        gen_pipe = pipeline("text-generation", model=mdl, tokenizer=tok, max_new_tokens=512, do_sample=False)
        print(f"Using local Hugging Face pipeline with model: {local_model_id}")
        return HuggingFacePipeline(pipeline=gen_pipe)
    except Exception as e:
        print(f"Local pipeline initialization failed: {e}")
        raise # Re-raise the exception to make it visible

# ======================================================
# Step 9: Start bot (select topic + process video)
# ======================================================

def start_bot(video_input: str, selected_topic: str):
    global qa_chain, current_topic, conversation_ended
    current_topic = selected_topic
    try:
        transcript = fetch_full_transcript(video_input)
        qa_chain = build_qa_chain(transcript)
        memory.clear()
        conversation_ended = False
        return f"Video processed under topic: {current_topic}"
    except Exception as e:
        return f"Error: {str(e)}"

# ======================================================
# Step 10: Chat with Bot (loop until 'exit')
# ======================================================

def chat_with_bot(user_input: str):
    global qa_chain, conversation_ended

    if user_input.strip().lower() in ["exit", "quit"]:
        conversation_ended = True
        return [("You", user_input), ("Bot", "Session ended. Type a new URL to start over.")]

    if qa_chain is None:
        return [("System", "Please process a video first.")]

    # Step #6: Answer using external knowledge (transcript via retriever)
    response = qa_chain.invoke({"question": user_input})

    # Rebuild a [(speaker, text)] list from memory to display in Gradio
    chat_history = []
    for m in memory.chat_memory.messages:
        name = m.__class__.__name__
        if name == "HumanMessage":
            chat_history.append(("You", m.content))
        elif name == "AIMessage":
            chat_history.append(("Bot", m.content))
    return chat_history

# ======================================================
# Step 11: Gradio UI
# ======================================================
with gr.Blocks(css=".gr-chatbot {height: 400px} .gr-textbox {height: 40px}") as demo:
    gr.Markdown("### YouTube Video Q&A Bot (RAG + Hugging Face + Whisper + Chroma)")

    topic_dropdown = gr.Dropdown(
        label="Choose Topic",
        choices=["Gen-AI", "Healthcare", "Data Engineering", "Podcast", "Other"],
        value="Gen-AI",
    )
    url_input = gr.Text(label="YouTube Video URL or ID")
    process_btn = gr.Button("Process Video")
    status_box = gr.Textbox(label="Status", interactive=False)

    chatbot = gr.Chatbot()
    user_input = gr.Textbox(label="Ask a Question")
    send_btn = gr.Button("Send")
    logout_btn = gr.Button("Logout")

    process_btn.click(start_bot, inputs=[url_input, topic_dropdown], outputs=status_box)
    send_btn.click(chat_with_bot, inputs=user_input, outputs=chatbot)
    logout_btn.click(lambda: os._exit(0), inputs=[], outputs=[])

# ======================================================
# Step 12: Launch App
# ======================================================
if __name__ == "__main__":
    # share=True makes it easy to test remotely in Colab; remove if running locally
    demo.launch(share=True, debug=True)